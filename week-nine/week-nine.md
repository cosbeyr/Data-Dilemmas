### How Effective Are Security Warnings, How Ethical Are Security Warning Studies

The first paper presents a study identifying how effective or ineffective security warnings are. Unlike previous studies, they completed this study in the field rather than in a lab to gain more genuine insights as to how non-technical users interact with these warnings on a day to day basis. Given my knowledge of HCI and the ways researchers conduct these types of studies, this approach makes a lot of sense to me; users tend to behave differently when in an unfamiliar environment or when they are hyper aware of the fact that they are being observed. I appreciated the sections dedicated to ethics and method limitations in this publication, the authors made it very clear what was collected, how a user could manage what was going to be collected from them and how representative their sample was in the greater context. Their field approach and the privacy concerns that came with that limited the information they collected about the users they were observing; it would have been interesting for them to have a more official separation between more technical users and less technical users to see how the patterns of interaction with the warning differ. The analysis of their collected usage reveals that “technically skilled users ignore warnings more often, and warning frequency is inversely correlated with user attention” (Akhawe, 258). They defined “technically skilled” as users that used the Linux operating system. In my view, most “technically skilled” individuals can easily interpret the warning messages they receive and are able to employ their own judgement in determining malicious from non-malicious technology. This is a skill I do not think the non-technical group would possess which could make them more vulnerable. 

The authors’ discussion of warning fatigue, in which “users click through more-frequent errors more quickly” reminds me of an anomaly detection research project I contributed to a few years ago. We were designing technology for security analysts’ use in determining anomalous user behavior on a corporate network. There is a plethora of technology available to security analysts that can lead to an abundance of false positives. The general thinking is that false positives are better than false negatives but this can lead the analysts to become desensitized or fatigued to the alerts provided by the anomaly detection and other security programs they employ which is similar to the predicament faced by average users with browser warnings. Another comparison between analysts and average users can be drawn in the context of targeted attacks. The “boiling frog” problem in security analysis is when a bad actor slowly ramps up an attack to trick a security system into believing it is benign. I imagine the same can be true for an end user; if a bad actor chose to target them specifically with malicious links, even if the end user did their due diligence to initially investigate the warning message given to them, if they chose to trust the first link they could also choose to trust additional and more malicious links from the same bad actor. 

The second article focused on designing and implementing attractors which are “user interface elements designed for attracting users’ attention to critical information in a security-decision dialog” (Braco-Lillo, 1). Unlike the previous article, these authors relied on Mechanical Turk workers as participants. This is less of an “in the wild” investigation because the participants were more readily aware that they were taking part in this study. I appreciated that the authors included information about the participants. Throughout the three experiments, the demographic markers were relatively the same; I thought it was interesting that they narrowed their participant pool to those who designated themselves as “students” or “unemployed.” It would have been nice if the authors had included discussion about why they chose the target demographics that they did, where the participants were located regionally or more information about the biases or limitations that came with their participant pools. Although they generally found benefit with their user interface attractors, namely a reduced risk that a user would install illegitimate software, grant excessive permissions to external forces or fail to recognize a dialog instruction, they also note that “inhibitive attractors may discourage users from performing useful actions or delay their workflow” (11). I think it would be interesting for the Google researchers from the first paper to employ the attractors presented in the second paper to see how these changes affected user interaction with browser warnings in the wild.


---

Akhawe, Devdatta, and Adrienne Porter Felt. "Alice in Warningland: A Large-Scale
Field Study of Browser Security Warning Effectiveness." *USENIX Security
Symposium,* 2013, pp. 257-272.

Bravo-Lillo, Cristian et al. "Your Attention Please." *Symposium on Usable Privacy and
Security,* 2013.

---
[Go Back](https://cosbeyr.github.io/Data-Dilemmas/)
