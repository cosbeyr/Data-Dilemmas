### Privacy is a Power We Constantly Give Up

Cheshire argues that “those who show that they are willing to take a chance
online open themselves to risk; however, the long-term payoff is a kind of
online social intelligence that rewards risk-taking with new opportunities”
(53). I would have liked for “online social intelligence” to be further defined
in this work. I see a few different interpretations but I’m not convinced of how
valuable any of them truly are. If “new opportunities” is equated to more online
interaction with other entities, how do other entities know that you have taken
this risk? How is the information that you have taken this risk and achieved
this social intelligence propagated to others? It makes more sense to me that
“new opportunities” only applies within the context of the current person or
entity you are taking the risk with but this seems like it requires a lot of
time and a lot of risk to build up that kind of trust with someone else on the
internet. In the long term if both parties are taking these risks then there
exists a mutual trust between the two.  Later on, Cheshire confirms this idea by
stating that “cooperation over time leads to larger, higher-risk entrustments”
(53). I worry that if a bad actor were to play a long game they could gain the
trust of another user with these lower stakes risks and lead that user to take
bigger and bigger risks with them until the user has risked the thing that they
were initially after -- it reminds me of the boiling frog concept. The only
negative for them in this scenario is that, at the end, they lose your trust and
therefore lose the potential business, communication or collaboration which may
not matter to them given the number of internet users out there for them to take
advantage of. The fact that it is harder for humans to be considered
“trustworthy” on the Internet as opposed to in person explains why the concept
of “assurance” has become more popular. Users want another entity to decide who
they should and should not trust.

Cheshire goes on to say that “individuals have a strong incentive to act
cooperatively when robust monitoring and assurance structures are present, but
cooperation in these cases has more to do with sanctions and other negative
outcomes than interpersonal trust” (54). Online assurance shifts the
trustworthiness check from the users themselves to the intermediary link between
them (a technology system or company); the parties in question essentially lower
their guard because they think the assurance system is infallible when really it
is only deceivingly convenient. Companies make it too easy for users to place
their trust into assurance structures presumably because it is economically
helpful to the company; there exists a lack of user understanding surrounding
the damage that this can potentially cause and it’s not a good design. Part of
this problem stems from the fact that “people tend to treat bots and information
agents as if they are human, even when we clearly know they are not” (55). There
has been a lot of interdisciplinary work to assess the ways humans view robots
and technology. In one project, researchers told their subjects to come to a
location to complete a survey and when they arrived, they were greeted and led
down the hall by a robot. While they were completing the survey, a fire alarm
began to sound, staged smoke crept in and the robot appeared to lead the subject
to safety. What the researchers found was that the subjects followed the robot
even when the robot led them into potential harm (e.g. an open window, a dark
and smoky garage). I wonder to what extent  “virtual assistants” like Alexa and
Siri influence our perception. By personifying our technology are we making it
easier for users to put their trust into assurance systems because they
unknowingly misinterpret them in the same way they would another human? 

From Cheshire’s publication, it seems to me that we are putting too much trust
into technology and, in doing so, decreasing the power we have over our privacy.
The “risks” we take lead us to give our personal information to bots associated
with large companies. Boyd states that “our interpreted selves aren’t simply the
product of our own actions and tastes; they’re constructed by recognizing
similar patterns across millions of people” (Boyd, 349). This makes me think
that by putting our trust into assurance systems, we are not only opening
ourselves up to harm but also others. It’s not news to anyone that large
companies are constantly collecting data from us. I think motivation is really
important when considering if this is ethical or not. It makes sense to me that
a social scientist would want to better understand humans and human action; it
does not make sense to me for a company to pursue this type of information for
their own monetary gain. I think that the economics gets in the way of
protecting users and their argument that they want to better serve their users
by understanding them is only a half truth which becomes apparent when you think
about how often personal data is sold from these types of companies. I thought
Boyd’s points on “social steganogrpahy” as an alternative to online privacy
settings was an interesting approach for users to regain control of their data.
It does not necessarily get at the root of the problem which I deem to be the
algorithms used to collect the data and find patterns which Boyd points to in
saying that users are “still subject to advertising and personalization based on
what they post” (349). I’m curious how the algorithms and models will adapt to
nonsensical input. Regardless, I think it is important that we introduce ways
for users to challenge how they have been interpreted by these models. This also
points to objective - should we be building this model? Is this model’s purpose
ethical and in benefit of society? There is always the potential for harm,
especially when we so often focus on the patterns a model finds and throw out
the outliers that don’t fit those groupings. 


---

Boyd, D. "Networked Privacy." *Surveillance & Society,* vol. 10 (3/4), 2012, pp.
348-350.

Cheshire, Coye. "Online Trust, Trustworthiness, or Assurance?" *Daedalus, the
Journal of the American Academy of Arts & Sciences,* vol. 140 (4), 2011, pp.
49-58.




---
[Go Back](https://cosbeyr.github.io/Data-Dilemmas/)
