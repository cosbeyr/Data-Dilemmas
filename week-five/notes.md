### Cheshire
"signalling one's intentions through an initial act of risk-taking is critical
  in online interactions when incomplete information ... makes it difficutl to
assess the trustworthiness of others" (52) 
- This to me seems like a double edged sword, is the importance of this negated
  with the presence of bad actors? This may be where assurance is necessary --
there are situations in which we cannot be fully sure of another party's
trustworthiness. This explains why assurance has begun to play a bigger role but
I want to argue that it is not the right answer.
- I guess the "bad actors" I referring to are the "risk" Cheshire references

"Those who show that they are willing to take a chance online open themselves to
risk; however, the long-term payoff is a kind of online social intelligence that
rewards risk-taking with new opportunities" (53)
- What are these new opportunities? What is meant by "social intelligence"? Is
  it a personal thing, like the more risks you take on the internet the more you
understand it? How is the fact that you took a risk propagated to others to
produce this payoff?
- Maybe this is referencing the "time" thing. The fact that you took a risk is
  not propagated to several others, just the single person or entity you are
interacting with. Over time they deem you trustworthy because you have
taken this risk? I guess in the long term if both parties are taking these risks
then there exists mutual trust but I think if a bad actor were to play a
longgame they could gain your trust with these low stakes risks and lead you to
take a bigger risk to profit them. The only negative of this for them is that
they lose your trust and therefore lose the potential
business/communication/collaboration between the other party.
    - "cooperation over time leads to larger, higher-risk entrustments" (53)
- This idea of mutual trust must be what they mean by "cooperation"

"Individuals have a strong incentive to act cooperatively when robust monitoring
and assurance structures are present, but cooperation in these cases has more to
do with sanctions and other negative outcomes than interpersonal trust" (54)
- this brings me back to the conversation last week about regulation: it's
  penalty rather than morality
- When considering two parties online assurance shifts trustworthiness check to
  the intermediary link between them (system or company), the parties
essentially lower their guard because they think assurance is infallible
- "The key implication is that if assurances fail, cooperation and trust will
  fail as well" (54) 
- it's convenient for a user to place their trust in assurance structure,
  companies make it too easy because it is economically helpful to the company?
- There exists a lack of understanding in the damage that this has
- I don't think it's a good design

"people tend to treat bots and information agents as if they are human, even
when we clearly know they are not" (55)
- lots of research on this topic
- our relationship with technology is evolving
- is this a potential flaw with AT? does abstracting humans and technology and
  ideas as "actors" further blur this line?

"When a system 'betrays' a human's trust, assigning blame can have enormous
repercussions: should we blame the system itself, those who programmed the
system, the organization that hosts the system, a quality control person, the
director of the organization, or the organization in general?" (55)
- My answer was that we should blame everyone but now I'm not sure that that is
  feasible
- This is a reason why corporate entities have been so successful, the blame
  falls on so many that there isn't an easy way to fight back 
- These assurance systems are abstracted versions of all of the aforementioned
  groups' points of view which, to me, makes them seem even less like a good
answer
- "trust placed in information systems often has less to do with the actual
systems and more to do with the humans, organizations, and governments that
maintain or control them" (56)
    - This is what I mean. It's not the systems that we need to focus on, it's
      the people involved with designing them. Their motivations are not often
in the best interests of the usres.

"facilitate interpersonal interaction and embrace the complexity of emergent
social uses of online information technologies" (57)
- This is the way we need to be "designing online communication technologies" in
  order to promote the motivations of the collective userbase and promote
cooperation

* People are always going to be an essential part of how we gain trust
virtually. I don't think we should focus on shifting reliance towards assurance
systems. But the issue is that it's getting harder and harder to trust other
users on the internet (bots, bad actors). The time and the vulnerability make
sense - interacting with someone over a long period of time and them sharing
things with me like photos makes me trust them more. 
* Technology to generate voices and photos is scare and degrading of this trust


### Boyd

"Our interpreted selves aren't simply the product of our own actions and tastes;
they're constructed by recognizing similar patterns across millions of people"
(349)

"We are the product of the people we know and the socio-cultural environment in
which we are situated" (349)
- Social scientists who want to understand humans better are well-motivated but
  corporations interested in the monetary value of understanding humans better
are morally corupt

"Social steganography" - alternative to online privacy settings
- An interesting approach to gaining back control 
- Does not get at the root of the problem which is algorithms (they are the
  nosiest parents of all)
- "still subject to advertising and personalization based on what they post" 
- Does this add to model noise? How does the model adapt to this nonsensical
  input?

"alternative models for dealing with networked privacy ... shifting to a model
that focuses on usage and interpretation" (349)
- This is important to me. What Boyd says about introducing mechanisms for users
  to challenge how they have been interpretted is important. I think an
effective way to represent all humans with one model is to allow for the human's
input. This also points to objective - why is this model being built? What is
the purpose of it? Is that purpose an ethical one that will benefit society? 

"categories become untenable as an organizational structure because people's
lives are interwoven with others' lives" (350)
- The actions of one person affects the actions of many others
- Building a model to represent groups of people has the potential to be harmful 
    - Especially when you only go by what the model says and throw away the
      outliers 
    - Especially when you abstract the person into a vector representation
- Is it fair for us to make decisions about someone based on data about other
  people?


###
